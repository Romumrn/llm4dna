{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azTfbLsebKly"
   },
   "source": [
    "# **Tutorial GROVER - DNA Language Model**\n",
    "\n",
    "Melissa Sanabria, Jonas Hirsch, Pierre M. Joubert, Anna R. Poetsch\n",
    "\n",
    "Biomedical Genomics, Biotechnology Center, Center for Molecular and Cellular Bioengineering, Technische Universität Dresden  \n",
    "melissa.sanabria@tu-dresden.de arpoetsch@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEYEy5gpTuq4"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial will show you how to use the DNA language model GROVER to perform fine-tuning tasks to investigate genome biology.\n",
    "\n",
    "[GROVER](https://www.nature.com/articles/s42256-024-00872-0) (\"Genome Rules Obtained Via Extracted Representations\") is a foundation DNA language model with an optimized vocabulary for the human genome. It has been pre-trained on the human genome and needs to be trained a second time, or fine-tuned, to perform specific tasks.\n",
    "   \n",
    "   \n",
    "For this tutorial we chose an end-to-end example to fine-tune the pre-tained GROVER to predict DNA binding by the CCCTC-Binding Factor (CTCF). The task is to recognize which sites that contain a CTCF binding motif are actually bound by the protein. We will be using ChIP-seq data from HepG2 cells obtained from ENCODE.  \n",
    "\n",
    "Throughout this tutorial, you will find ***MODIFY*** signs that indicate pieces of code that are task specific, meaning that they are specific for CTCF binding site prediction, and you can modify them for your desired task.\n",
    "\n",
    "**WARNING**: Even though Google Colab gives you access to a GPU, which is necessary for this computationally expensive work, the free version only gives you access to a very simple device and the training might take a while.\n",
    "\n",
    "In the version of Colab, which is free of charge, notebooks can run for a maximum of 12 hours, depending on availability and your usage patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tLHIRp6TxbR"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, save a copy of this notebook in your Google Drive by navigating to 'File' then 'Save a copy in Drive...'.\n",
    "\n",
    "Once you've opened your own copy, make sure you have enabled the GPU runtime for Google Colab by navigating to the menu 'Runtime', select 'Change runtime type' and set the runtime to 'T4 GPU'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX13rpTxUOtc"
   },
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "kages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install pandas scikit-learn transformers torch\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yeDpnVpsRW-v"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwget\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, Trainer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCq5uQdrRW-y"
   },
   "source": [
    "# Prepare Data\n",
    "\n",
    "**MODIFY!**\n",
    "Create Dataset with cancer variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTRzJAc2ZFVZ"
   },
   "source": [
    "# Create dataset\n",
    "\n",
    "From now on, we come back to general instructions that apply for any fine-tuning task.\n",
    "\n",
    "Please remember that to do this yourself you will need a data frame with at least two columns: label, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier VCF téléchargé avec succès !\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Télécharger les variants pathogènes de ClinVar\n",
    "url = \"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\"\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "with open(\"clinvar_cancer.vcf.gz\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"Fichier VCF téléchargé avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 800 variants with risk factor.\n",
      "🔍 Randomly selected 200 variants.\n",
      "📌 Processing variant 7:44152381 T>A\n",
      "📌 Processing variant 12:40340400 G>A\n",
      "📌 Processing variant X:83874085 G>T\n",
      "📌 Processing variant 4:6302355 C>T\n",
      "📌 Processing variant 17:37731730 T>C\n",
      "📌 Processing variant 4:83267659 A>G\n",
      "📌 Processing variant 7:44149813 G>A\n",
      "📌 Processing variant 10:71819104 T>G\n",
      "📌 Processing variant 5:112839514 T>A\n",
      "📌 Processing variant 11:17387583 T>C\n",
      "📌 Processing variant 4:163148340 C>A\n",
      "📌 Processing variant 3:9944556 G>A\n",
      "📌 Processing variant 3:44775355 C>T\n",
      "📌 Processing variant 20:59301614 A>AG\n",
      "📌 Processing variant 3:30644840 A>G\n",
      "📌 Processing variant 1:21290752 G>T\n",
      "📌 Processing variant 12:6018667 T>C\n",
      "📌 Processing variant 2:232791426 A>G\n",
      "📌 Processing variant 1:54032127 C>T\n",
      "📌 Processing variant 6:31575186 G>A\n",
      "📌 Processing variant 11:2159893 T>A\n",
      "📌 Processing variant 17:78133937 CT>C\n",
      "📌 Processing variant 1:161039733 G>A\n",
      "📌 Processing variant 10:43109146 C>A\n",
      "📌 Processing variant 9:22125504 G>C\n",
      "📌 Processing variant 7:44152379 C>A\n",
      "📌 Processing variant 3:30606635 T>G\n",
      "📌 Processing variant 7:44153385 C>G\n",
      "📌 Processing variant 10:122461028 G>A\n",
      "📌 Processing variant 10:58702095 C>T\n",
      "📌 Processing variant 3:138174747 A>G\n",
      "📌 Processing variant 20:44401428 T>C\n",
      "📌 Processing variant 7:44149807 A>T\n",
      "📌 Processing variant 15:90088606 C>A\n",
      "📌 Processing variant 2:24820051 TG>T\n",
      "📌 Processing variant 4:108019599 C>T\n",
      "📌 Processing variant 4:6302427 G>A\n",
      "📌 Processing variant 2:24820010 CAAG>C\n",
      "📌 Processing variant X:23393312 C>CA\n",
      "📌 Processing variant 10:34450451 A>G\n",
      "📌 Processing variant 15:90088605 C>G\n",
      "📌 Processing variant 6:31948443 A>G\n",
      "📌 Processing variant X:154400825 C>A\n",
      "📌 Processing variant 3:44831003 TTC>T\n",
      "📌 Processing variant 10:68882104 T>C\n",
      "📌 Processing variant 17:47731462 T>C\n",
      "📌 Processing variant 13:43859494 C>A\n",
      "📌 Processing variant 7:44152279 C>T\n",
      "📌 Processing variant 20:63695093 A>C\n",
      "📌 Processing variant 7:44146578 C>A\n",
      "📌 Processing variant 17:37710529 T>C\n",
      "📌 Processing variant 22:36265812 A>G\n",
      "📌 Processing variant 15:90231093 A>AC\n",
      "📌 Processing variant 1:93909753 G>A\n",
      "📌 Processing variant 16:72997275 A>.\n",
      "📌 Processing variant 9:137449656 T>C\n",
      "📌 Processing variant 22:36265860 A>G\n",
      "📌 Processing variant 18:48927093 T>.\n",
      "📌 Processing variant 13:45337574 A>G\n",
      "📌 Processing variant 14:35292469 C>G\n",
      "📌 Processing variant 2:128268430 C>T\n",
      "📌 Processing variant 19:18388639 T>G\n",
      "📌 Processing variant 4:6302058 T>C\n",
      "📌 Processing variant 1:32204044 C>T\n",
      "📌 Processing variant 17:37710652 T>C\n",
      "📌 Processing variant 20:50582818 C>CG\n",
      "📌 Processing variant 16:50700122 G>A\n",
      "📌 Processing variant 14:65078020 AT>A\n",
      "📌 Processing variant 1:223111257 T>C\n",
      "📌 Processing variant 11:2160955 C>T\n",
      "📌 Processing variant 3:48641910 C>T\n",
      "📌 Processing variant 4:108014530 G>A\n",
      "📌 Processing variant 2:69152193 G>A\n",
      "📌 Processing variant 15:66703871 T>A\n",
      "📌 Processing variant 11:5225284 C>T\n",
      "📌 Processing variant 14:94100772 G>A\n",
      "📌 Processing variant 12:49853685 G>A\n",
      "📌 Processing variant 1:156135917 C>T\n",
      "📌 Processing variant 4:6295055 G>A\n",
      "📌 Processing variant 13:27920262 C>T\n",
      "📌 Processing variant 19:44908684 T>C\n",
      "📌 Processing variant 2:182834857 G>C\n",
      "📌 Processing variant 4:6300676 A>T\n",
      "📌 Processing variant 4:107989979 C>T\n",
      "📌 Processing variant 19:1047508 AGGAGCAG>A\n",
      "📌 Processing variant 22:46409760 T>TCA\n",
      "📌 Processing variant 19:49807348 C>A\n",
      "📌 Processing variant 13:109784115 G>C\n",
      "📌 Processing variant 7:44145521 C>A\n",
      "📌 Processing variant 6:25862334 T>G\n",
      "📌 Processing variant 4:6291919 G>A\n",
      "📌 Processing variant 4:6301632 T>G\n",
      "📌 Processing variant 20:10412542 T>G\n",
      "📌 Processing variant 6:125076745 G>A\n",
      "📌 Processing variant 21:33335578 A>C\n",
      "📌 Processing variant 2:182838608 G>A\n",
      "📌 Processing variant 17:37710561 TGCTGTAAAACCGACTGGCTGGTCACCATG>T\n",
      "📌 Processing variant 12:10159692 A>.\n",
      "📌 Processing variant 13:27924341 G>T\n",
      "📌 Processing variant 15:73929861 T>C\n",
      "📌 Processing variant 12:120999299 G>A\n",
      "📌 Processing variant 9:33403472 T>C\n",
      "📌 Processing variant 4:6300987 G>A\n",
      "📌 Processing variant 7:44153402 C>T\n",
      "📌 Processing variant 12:112472980 C>G\n",
      "📌 Processing variant X:23393644 TC>T\n",
      "📌 Processing variant 10:43102410 G>T\n",
      "📌 Processing variant 13:37047903 T>A\n",
      "📌 Processing variant 1:160425122 T>C\n",
      "📌 Processing variant 5:72196449 G>T\n",
      "📌 Processing variant 5:1254388 AGTGGCAC>A\n",
      "📌 Processing variant 2:240603286 C>T\n",
      "📌 Processing variant 12:120989005 G>A\n",
      "📌 Processing variant 12:120996707 C>T\n",
      "📌 Processing variant 17:49404263 G>A\n",
      "📌 Processing variant 10:43118381 T>C\n",
      "📌 Processing variant 3:138167206 G>A\n",
      "📌 Processing variant 3:57096409 G>A\n",
      "📌 Processing variant 6:160716688 G>A\n",
      "📌 Processing variant 3:30650316 C>A\n",
      "📌 Processing variant 5:1282488 C>A\n",
      "📌 Processing variant 6:35639794 T>C\n",
      "📌 Processing variant 8:13099960 G>A\n",
      "📌 Processing variant 11:2160956 G>A\n",
      "📌 Processing variant 5:14690245 TG>T\n",
      "📌 Processing variant 3:30606547 T>A\n",
      "📌 Processing variant 11:124896308 C>G\n",
      "📌 Processing variant 6:166160758 G>A\n",
      "📌 Processing variant 14:61457521 G>A\n",
      "📌 Processing variant 4:6301836 A>G\n",
      "📌 Processing variant 10:17849701 G>A\n",
      "📌 Processing variant 17:31163338 C>A\n",
      "📌 Processing variant 6:42925264 CA>C\n",
      "📌 Processing variant 2:240591757 G>A\n",
      "📌 Processing variant 15:90233705 T>C\n",
      "📌 Processing variant 3:169764740 G>GGCTGACA\n",
      "📌 Processing variant 1:78660577 A>AC\n",
      "📌 Processing variant 19:6697673 G>C\n",
      "📌 Processing variant 2:128268188 T>C\n",
      "📌 Processing variant 12:66203804 G>A\n",
      "📌 Processing variant 9:21994142 G>GTCT\n",
      "📌 Processing variant 5:14690181 A>T\n",
      "📌 Processing variant 17:40632336 T>TG\n",
      "📌 Processing variant 17:21703883 A>G\n",
      "📌 Processing variant 15:66781456 G>A\n",
      "📌 Processing variant 3:69964940 G>A\n",
      "📌 Processing variant 16:14604177 CT>C\n",
      "📌 Processing variant 11:502123 C>A\n",
      "📌 Processing variant 2:74368870 A>G\n",
      "📌 Processing variant 16:89920138 G>C\n",
      "📌 Processing variant 17:80313099 C>T\n",
      "📌 Processing variant 12:48981230 C>T\n",
      "📌 Processing variant 12:120994255 G>T\n",
      "📌 Processing variant 6:31593133 C>G\n",
      "📌 Processing variant 7:44150021 G>C\n",
      "📌 Processing variant 19:610364 G>A\n",
      "📌 Processing variant 15:73927241 G>A\n",
      "📌 Processing variant 20:45314116 C>T\n",
      "📌 Processing variant 16:10877045 G>.\n",
      "📌 Processing variant 6:52438538 A>G\n",
      "📌 Processing variant 6:29945521 A>T\n",
      "📌 Processing variant 11:2160946 G>C\n",
      "📌 Processing variant 1:196737575 T>A\n",
      "📌 Processing variant 6:31948042 C>G\n",
      "📌 Processing variant 3:30606876 G>A\n",
      "📌 Processing variant 7:44145627 A>T\n",
      "📌 Processing variant 22:28800769 G>C\n",
      "📌 Processing variant 11:498935 T>G\n",
      "📌 Processing variant 12:2236129 G>A\n",
      "📌 Processing variant 10:113047288 G>C\n",
      "📌 Processing variant 1:78635486 T>A\n",
      "📌 Processing variant 12:10160849 C>G\n",
      "📌 Processing variant 3:30606540 C>T\n",
      "📌 Processing variant 17:37744546 C>T\n",
      "📌 Processing variant 3:30644768 C>A\n",
      "📌 Processing variant 5:132657117 C>T\n",
      "📌 Processing variant 11:92969849 G>C\n",
      "📌 Processing variant 4:107990050 G>A\n",
      "📌 Processing variant 5:23023985 G>T\n",
      "📌 Processing variant 4:41260751 C>G\n",
      "📌 Processing variant 20:23049347 T>C\n",
      "📌 Processing variant 7:44153411 A>C\n",
      "📌 Processing variant 3:129433859 G>A\n",
      "📌 Processing variant 12:120999349 C>T\n",
      "📌 Processing variant 21:33436933 G>T\n",
      "📌 Processing variant 2:63487491 C>T\n",
      "📌 Processing variant 4:6277525 C>T\n",
      "📌 Processing variant 7:34778501 A>T\n",
      "📌 Processing variant 20:56249736 T>G\n",
      "📌 Processing variant 13:45337514 A>C\n",
      "📌 Processing variant 12:40315090 T>A\n",
      "📌 Processing variant 1:155235843 T>C\n",
      "📌 Processing variant 3:10289835 C>T\n",
      "📌 Processing variant 4:6277538 A>G\n",
      "📌 Processing variant X:71167508 C>T\n",
      "📌 Processing variant 12:52516824 C>T\n",
      "📌 Processing variant 4:6301978 G>A\n",
      "📌 Processing variant 1:160814740 CAT>C\n",
      "📌 Processing variant 4:6289053 G>T\n",
      "📌 Processing variant 20:7125642 T>C\n",
      "✅ Cancer variant sequences with risk factor saved to cancer_variants.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Mismatch: Expected TGGAAGAAACATTGCCAGACGTGGCCATTTGCAAATGTCGTTTCCTGGTCTGTGTCTTTTGATTCTCCTGACAGCATATTTGAGTTAATTTTTTAATGTGTAAGGTATGAGTGGAGGTCCTCATTAAAAATTGTTTAGGCTGGCTGTATGGCTCATGCCTGTAATCACAGCACTTTGAGAGGCTGAAGTGGGAGGATCATTTGAGCCCAGGAGTTTGAGACCGGCTGGGCTAACATGGCGAGACCCCGTCTCTACAAAAAATAAGAAAATTAGCTGGGTGTGGCACCACGCATCTGTAGTCTCAGTTGTACAGGAGGCTGAGGCCAGAGGATCACTTGAGCCCAGGAGATTGAGGCTGCAGTGAGCCATGATTGCAGTAAGATGTGATCACATCCAGTTACATTTGATTTTCAGATAAACAACAGAAGTTTAGTGGAGGCTGGGGGCAGTGGCTCACAGCTGTAATCCCAGCACTTTAGGAAGCTGAGGCAGGTGGATCACCTGAGGTCAGGAGTTCGAGACCAGCCTGGCCAATATGATGAAACCGTGTCTCTACTAAAAATACAAAAAAATTAGCCGGGCGTGGTGGCGGGCGCCTATAATCCCAGCTACTCAGAAGGCTGAGGCAGGAGAATCGCTTGAACCCGGGAGACGGAGGTTGCAGTGAGCCAAGATTGTGCTGCTGCACTCCAGCCTGGGTGATAAGAGTGAAACTCCATCTCAAAAAAAAAAAAAAAAAAAAAACAAATTGGGGTTGTCTGTCTTTTCTCATTGTTTCCATTGTTTCTGGATCTGTGTGTCAGAGATGTGTGCTGCACACATCCTTTCTCCGCCTAGCTTGGTTTTCACTCTTGGTGGTCGCAGACATCTTGGGGGCTACTTGAAGGAACTCTAAGCCGACCAGAGGCTGGGCTGGACTCGCCGTTTGGCGCCCAGCCGGAGACCCCAGGGCCACCCTCCCCGGTTCCATCCTGGGAGAGAAGAAACTTCCACTGTCCCAGTCTCTCCTGTGGTGACTTCTTTTGCTCAAGGGTTCACAAGGGCAGCAGACTGATCTGCACAAGAGGGACTGAGCCTGGCTGCGGCCGCACCTGATGCCTGTGCACACCTGTCCCTGGTGGTCTCTCCTGTGGTGACTTCTTTTGCTCAAGGGTTCACAAGGGCAGCAGACTGATCTGCACAAGAGGGACTGAGCCTGGCTGCGGCCGCACCTGATGCCTGTGCACACCTGTCCCTGGTG but found TGGAAGAAACATTGCCAGACGTGGCCATTTGCAAATGTCGTTTCCTGGTCTGTGTCTTTTGATTCTCCTGACAGCATATTTGAGTTAATTTTTTAATGTGTAAGGTATGAGTGGAGGTCCTCATTAAAAATTGTTTAGGCTGGCTGTATGGCTCATGCCTGTAATCACAGCACTTTGAGAGGCTGAAGTGGGAGGATCATTTGAGCCCAGGAGTTTGAGACCGGCTGGGCTAACATGGCGAGACCCCGTCTCTACAAAAAATAAGAAAATTAGCTGGGTGTGGCACCACGCATCTGTAGTCTCAGTTGTACAGGAGGCTGAGGCCAGAGGATCACTTGAGCCCAGGAGATTGAGGCTGCAGTGAGCCATGATTGCAGTAAGATGTGATCACATCCAGTTACATTTGATTTTCAGATAAACAACAGAAGTTTAGTGGAGGCTGGGGGCAGTGGCTCACAGCTGTAATCCCAGCACTTTAGGAAGCTGAGGCAGGTGGATCAC\n",
      "📌 Processing variant 1:54032127 C>T\n",
      "📌 Processing variant 17:37731657 G>C\n",
      "📌 Processing variant 5:147828115 T>C\n",
      "📌 Processing variant 17:80313099 C>T\n",
      "📌 Processing variant 11:2160901 G>T\n",
      "📌 Processing variant 17:37744560 C>G\n",
      "📌 Processing variant 4:186083922 G>T\n",
      "📌 Processing variant 6:35675286 G>A\n",
      "📌 Processing variant 4:6302093 C>A\n",
      "📌 Processing variant 8:38414611 A>T\n",
      "📌 Processing variant 11:44310044 C>A\n",
      "📌 Processing variant 1:11791276 C>T\n",
      "📌 Processing variant 15:90230938 C>CAA\n",
      "📌 Processing variant 4:6302355 C>T\n",
      "📌 Processing variant 13:45337574 A>G\n",
      "📌 Processing variant 1:161662856 G>C\n",
      "📌 Processing variant 7:44147704 A>G\n",
      "📌 Processing variant 1:207767135 GAC>G\n",
      "📌 Processing variant 14:24164955 G>A\n",
      "📌 Processing variant 12:120999299 G>A\n",
      "📌 Processing variant 2:24824537 C>T\n",
      "📌 Processing variant 3:9944556 G>A\n",
      "📌 Processing variant 15:73929861 T>C\n",
      "📌 Processing variant 12:120994255 G>T\n",
      "📌 Processing variant 17:40632336 T>TG\n",
      "📌 Processing variant 16:53767042 T>C\n",
      "📌 Processing variant 11:17387407 C>T\n",
      "📌 Processing variant 7:44153385 C>G\n",
      "📌 Processing variant 4:46337053 T>A\n",
      "📌 Processing variant 11:2160956 G>A\n",
      "📌 Processing variant 4:163164331 AGAC>A\n",
      "📌 Processing variant 14:35292469 C>G\n",
      "📌 Processing variant 9:22125504 G>C\n",
      "📌 Processing variant 2:634905 T>C\n",
      "📌 Processing variant 22:37576621 G>A\n",
      "📌 Processing variant 3:47678197 C>T\n",
      "📌 Processing variant 19:49660741 G>A\n",
      "📌 Processing variant 4:83267659 A>G\n",
      "📌 Processing variant 1:196747260 C>G\n",
      "📌 Processing variant 7:83981346 AACATGCAGAACCAT>A\n",
      "📌 Processing variant 17:37710561 TGCTGTAAAACCGACTGGCTGGTCACCATG>T\n",
      "📌 Processing variant 12:106992962 T>G\n",
      "📌 Processing variant X:5903492 C>CA\n",
      "📌 Processing variant 22:36265812 A>G\n",
      "📌 Processing variant 20:45420214 GGAAA>G\n",
      "📌 Processing variant 7:44150018 TC>AA\n",
      "📌 Processing variant 1:228282010 CTGTCCCACGAGAGT>C\n",
      "📌 Processing variant 6:52454254 C>T\n",
      "📌 Processing variant 1:160565472 TA>T\n",
      "📌 Processing variant 12:64108967 C>T\n",
      "📌 Processing variant 1:247421341 G>C\n",
      "📌 Processing variant 1:156134914 C>T\n",
      "📌 Processing variant 3:30650332 A>T\n",
      "📌 Processing variant 4:186083725 C>T\n",
      "📌 Processing variant 19:49662076 C>T\n",
      "📌 Processing variant 2:69152193 G>A\n",
      "📌 Processing variant 19:1046955 G>T\n",
      "📌 Processing variant 17:37744739 G>C\n",
      "📌 Processing variant 4:109741074 T>A\n",
      "📌 Processing variant 19:1047508 AGGAGCAG>A\n",
      "📌 Processing variant 5:14678734 G>T\n",
      "📌 Processing variant 6:151842200 T>C\n",
      "📌 Processing variant 4:56911408 CAGTG>C\n",
      "📌 Processing variant 2:240591746 T>C\n",
      "📌 Processing variant 12:120994373 C>T\n",
      "📌 Processing variant 16:89919733 C>A\n",
      "📌 Processing variant 3:30644800 C>G\n",
      "📌 Processing variant 2:74530433 C>G\n",
      "📌 Processing variant 17:37733588 C>T\n",
      "📌 Processing variant 5:1268581 G>A\n",
      "📌 Processing variant 12:52949556 A>T\n",
      "📌 Processing variant 4:6300750 A>T\n",
      "📌 Processing variant 1:78635486 T>A\n",
      "📌 Processing variant 19:7670411 G>A\n",
      "📌 Processing variant 1:10297206 A>T\n",
      "📌 Processing variant 6:169234915 G>A\n",
      "📌 Processing variant 9:131140575 C>T\n",
      "📌 Processing variant 3:30650358 T>C\n",
      "📌 Processing variant 10:32908395 C>CT\n",
      "📌 Processing variant 3:174275474 T>C\n",
      "📌 Processing variant 8:22046599 T>C\n",
      "📌 Processing variant 12:6643575 C>CT\n",
      "📌 Processing variant 10:43109146 C>A\n",
      "📌 Processing variant 15:66781456 G>A\n",
      "📌 Processing variant 3:30650400 A>G\n",
      "📌 Processing variant 4:6302094 A>C\n",
      "📌 Processing variant 15:66781346 C>CTCCATCAAGG\n",
      "📌 Processing variant 5:1266497 G>C\n",
      "📌 Processing variant 11:92969849 G>C\n",
      "📌 Processing variant 13:45337514 A>C\n",
      "📌 Processing variant 20:14327076 C>A\n",
      "📌 Processing variant 20:23049347 T>C\n",
      "📌 Processing variant 10:58702095 C>T\n",
      "📌 Processing variant 18:48927093 T>.\n",
      "📌 Processing variant 3:16598169 T>C\n",
      "📌 Processing variant 5:1393745 TGTGGGGGCCCTGCATGCGTCCTGGGGTAGTACACGCTCCA>T\n",
      "📌 Processing variant 20:63695093 A>C\n",
      "📌 Processing variant 3:30650316 C>A\n",
      "📌 Processing variant 4:11406961 T>C\n",
      "📌 Processing variant 3:30650346 G>C\n",
      "📌 Processing variant 1:196747209 G>T\n",
      "📌 Processing variant 19:4817957 G>A\n",
      "📌 Processing variant 2:138002079 C>T\n",
      "📌 Processing variant 1:2027636 A>C\n",
      "📌 Processing variant 9:33403472 T>C\n",
      "📌 Processing variant 19:49807348 C>A\n",
      "📌 Processing variant 4:6300696 G>A\n",
      "📌 Processing variant 17:21703847 C>T\n",
      "📌 Processing variant 8:38415905 C>T\n",
      "📌 Processing variant 2:63437430 C>G\n",
      "📌 Processing variant 4:6301987 T>C\n",
      "📌 Processing variant 11:320772 A>G\n",
      "📌 Processing variant 1:27370346 G>T\n",
      "✅ Cancer variant sequences with risk factor saved to cancer_variants.tsv\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import requests\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# 📌 PARAMETERS\n",
    "VCF_FILE = \"clinvar_cancer.vcf.gz\"  # Your compressed VCF file\n",
    "TSV_OUTPUT = \"cancer_variants.tsv\"\n",
    "FLANK_SIZE = 500  # Number of nucleotides before/after the mutation\n",
    "SAMPLE_SIZE = 200  # Number of variants to process (0 for all)\n",
    "\n",
    "# 📌 1️⃣ READ VARIANTS FROM A COMPRESSED `.vcf.gz` FILE\n",
    "def read_vcf_gz(vcf_file):\n",
    "    \"\"\"Reads a compressed .vcf.gz file and extracts cancer variants with risk factor\"\"\"\n",
    "    variants = []\n",
    "    with gzip.open(vcf_file, \"rt\") as f:  # Open in text mode\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):  # Skip header lines\n",
    "                continue\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            chrom = fields[0]\n",
    "            pos = int(fields[1])\n",
    "            ref = fields[3]\n",
    "            alt = fields[4].split(\",\")[0]  # Take the first ALT allele\n",
    "            info = fields[7]  # INFO field\n",
    "\n",
    "            # Check if the variant has a risk factor in the CLNSIG field\n",
    "            if \"CLNSIG=\" in info:\n",
    "                clnsig = info.split(\"CLNSIG=\")[1].split(\";\")[0]\n",
    "                if \"risk\" in clnsig :\n",
    "                    variants.append((chrom, pos, ref, alt))\n",
    "    return variants\n",
    "\n",
    "# 📌 2️⃣ GET THE REFERENCE SEQUENCE AROUND THE VARIANT\n",
    "def get_sequence(chrom, pos, flank=500):\n",
    "    \"\"\"Fetches the reference DNA sequence ±500 nucleotides around the variant\"\"\"\n",
    "    start = max(1, pos - flank)\n",
    "    end = pos + flank\n",
    "    url = f\"https://rest.ensembl.org/sequence/region/human/{chrom}:{start}-{end}?content-type=text/plain\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text.strip(), start, end\n",
    "    else:\n",
    "        print(f\"❌ Error fetching sequence for {chrom}:{pos}\")\n",
    "        return None, None, None\n",
    "\n",
    "# 📌 3️⃣ INSERT MUTATION INTO THE SEQUENCE\n",
    "def mutate_sequence(sequence, ref, alt, flank=500):\n",
    "    \"\"\"Inserts mutation (SNP or Indel) into the reference DNA sequence\"\"\"\n",
    "    center = flank  # Position of the variant in the extracted sequence\n",
    "    if sequence[center:center + len(ref)] != ref:\n",
    "        print(f\"⚠ Mismatch: Expected {ref} but found {sequence[center:center + len(ref)]}\")\n",
    "        return None\n",
    "    mutated_sequence = sequence[:center] + alt + sequence[center + len(ref):]\n",
    "    return mutated_sequence\n",
    "\n",
    "# 📌 4️⃣ SAVE TO A TSV FILE\n",
    "def write_tsv(output_file, data):\n",
    "    \"\"\"Writes variant data to a TSV file\"\"\"\n",
    "    with open(output_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        writer.writerow([\"chromosome\", \"start_of_bin\", \"end_of_bin\", \"label\", \"sequence\"])\n",
    "        writer.writerows(data)\n",
    "\n",
    "# 📌 5️⃣ RUN THE FULL PIPELINE\n",
    "def process_variants(vcf_file, output_tsv, flank_size=500, sample_size=0):\n",
    "    \"\"\"Full pipeline to generate mutated sequences in TSV format\"\"\"\n",
    "    variants = read_vcf_gz(vcf_file)\n",
    "    print(f\"🔍 Found {len(variants)} variants with risk factor.\")\n",
    "\n",
    "    # Apply random sampling if requested\n",
    "    if sample_size > 0:\n",
    "        sample_size = min(sample_size, len(variants))\n",
    "        variants = random.sample(variants, sample_size)\n",
    "        print(f\"🔍 Randomly selected {sample_size} variants.\")\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    for chrom, pos, ref, alt in variants:\n",
    "        print(f\"📌 Processing variant {chrom}:{pos} {ref}>{alt}\")\n",
    "\n",
    "        # Get reference sequence\n",
    "        seq, start, end = get_sequence(chrom, pos, flank_size)\n",
    "        if not seq:\n",
    "            continue  # Skip this variant if error occurs\n",
    "\n",
    "        # Insert mutation\n",
    "        mutated_seq = mutate_sequence(seq, ref, alt, flank_size)\n",
    "        if not mutated_seq:\n",
    "            continue  # Skip if mutation failed\n",
    "\n",
    "        # Add both mutated and reference sequences\n",
    "        output_data.append([chrom, start, end, 1, mutated_seq])\n",
    "        output_data.append([chrom, start, end, 0, seq])\n",
    "\n",
    "    # Write to TSV file\n",
    "    write_tsv(output_tsv, output_data)\n",
    "    print(f\"✅ Cancer variant sequences with risk factor saved to {output_tsv}\")\n",
    "\n",
    "# 📌 RUN THE SCRIPT\n",
    "process_variants(VCF_FILE, TSV_OUTPUT, FLANK_SIZE, SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YJr-DtbYZaWr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m CTCF_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(TSV_OUTPUT, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m CTCF_dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "CTCF_dataset = pd.read_csv(TSV_OUTPUT, sep='\\t')\n",
    "CTCF_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGAGAGTGTTGCTTGGACCAGGGGCTGGCCATAAGGATGGAGAGAAGGAGGTGATTCTTATAACATCATGGGAGAATGCAGGTGTTGATGGATTGAATGTGAGAAAGGGATTCCCAGGCAGGACCCCTGCACTTGGTTTAATGAAATTCTTAATAATTTCTGAGCGAGGGGCCGTGTCTCTTCATTTTGCGCTGGGCTTCGCAGGTCTCTGGTTTTTGCAATGCTGGAGTGACGCCTCTGGCTCATATATCTGCCTGCCCCCACCCCCCAGCCCAATCTTTGCAAAGGGAATTGGGCAGTGGGCACATGGAGGTGGGGCTCTCTCTCGTGTTCATCCTGCGAGACTCAGGAGCCCCTCTCTTCCATTTTGCCCTAAATCCCAGCCTCTTACAATCATTTCTTGGTGTATCACGGGCGCATCCTCCTGGAAGACCCCGTCGGGCTTCTGCTTCTCCAGGATCAGCCATTTAACAGCCCCGCAGAGGACTTGGGAGTCGATGACGATGAGGTTGACAGCCAGAGAGAAGACCTTGACCACGTAGGCGGTCAGCCTGGAGTGGGCACAGAGCATGAGCCAATCGGCTCTGAGATCCAGAGACTGCCATGTCAACCAGCCAATGAGCGTGGGGGAGGGACCAGGGCCTGGCCCAGTTTGCCTGGTTTGCCTTCCCAGGCCCCAGGACCCAGCTGTTGTATGCGGTCCGTGCTTAAGGATGCTTAATGACCGCCGGCCACTCAGCCAGCGCTTGCCTGGACTCTGCAGGTCCAGGGCTGTTTGGGCAAGGCTGGCCTAAGGGACCACCCCTGGCCAGGGTCCGGGCCCTGCTGGGGGTTGAGGGTGGGGAGTATGCATGGCCTGAGCTGGCTGTTGGGACTCACCAGGTGCTGGGTGCCCGTTTCACGAAGGCCGCAAAGGCAGAGCTGGGTTGTCTGAAGGCCAGCTGCTGGGTGTACCCTGCAGAGAAGAGAGAGGAACCCCCAAAAGAGCCGGGGCTGAGCAG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example sequence\n",
    "CTCF_dataset['sequence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMSntPD-a4aa"
   },
   "source": [
    "After getting our dataset, we need to split the samples into train, validation and test. We will go for the standard partitions, 80% for training, 10% for testing and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L0nvyRp2bCLZ"
   },
   "outputs": [],
   "source": [
    "train = CTCF_dataset.sample(frac=0.6, random_state=0)\n",
    "validation = CTCF_dataset.drop(train.index)\n",
    "test = validation.sample(frac=0.5, random_state=0)\n",
    "validation = validation.drop(test.index)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "validation = validation.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOZzimS0bN0N"
   },
   "source": [
    "# GROVER in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctsbs6iDZuFv"
   },
   "source": [
    "So far, we have not mentioned any Language Model terminology. Now, we need to change our sequence from nucleotides to tokens (or words). For this, we will download the tokenizer available in the higgingface project for GROVER. For this, we can use the model name *PoetschLab/GROVER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PMAT3KasqoEA"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n",
      "File \u001b[0;32m~/.conda/miniconda3/lib/python3.12/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/miniconda3/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/.conda/miniconda3/lib/python3.12/site-packages/transformers/utils/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     31\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/miniconda3/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py:40\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     43\u001b[0m BASIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsKECaMnany1"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"PoetschLab/GROVER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q91zgtv8bXEW"
   },
   "source": [
    "Then, we need to download the pre-trained GROVER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HE1FPPeuMj9L"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"PoetschLab/GROVER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J2_TXikc6CO"
   },
   "source": [
    "In order to make your work easier, we create a class to process the Dataset created for Grover.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AioLzAJpc6-F"
   },
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        sequences = [text for text in texts]\n",
    "\n",
    "        output = tokenizer(\n",
    "            sequences,\n",
    "            add_special_tokens=True,\n",
    "            max_length=310,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        self.input_ids = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFzOXFxBdTwk"
   },
   "outputs": [],
   "source": [
    "train_dataset = SupervisedDataset(train.sequence, train.label, tokenizer)\n",
    "test_dataset = SupervisedDataset(test.sequence, test.label, tokenizer)\n",
    "val_dataset = SupervisedDataset(validation.sequence, validation.label, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmty5erKdk9E"
   },
   "source": [
    "## Train model\n",
    "\n",
    "For the training process we choose:\n",
    "\n",
    "*   an Adam optimizer but you can explore the [optimizers](https://huggingface.co/transformers/v3.0.2/main_classes/optimizer_schedules.html) available in huggingface\n",
    "*   learning rate of 0.000001 since for fine-tuning tasks it is better to have a very low learning rate\n",
    "*   a total number of epochs of 4\n",
    "\n",
    "A higher number of epochs is ideal but Colab limits the amount of time you can run some code. We advice you to try with your own GPU resources with at least 10 epochs.\n",
    "\n",
    "In order to make your work easier, we create a method to compute five classification metrics. Here we will see some metrics such as accuracy, f1 score, precision and recall. You can explore the [sklearn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to choose other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuVwKF-2hP0z"
   },
   "outputs": [],
   "source": [
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(\n",
    "            labels, predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": matthews_corrcoef(\n",
    "            labels, predictions\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            labels, predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            labels, predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tTyIXo-dlNO"
   },
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(seed = 42,\n",
    "                               output_dir=\".\",\n",
    "                               per_device_train_batch_size=8,\n",
    "                               eval_strategy=\"epoch\",\n",
    "                               learning_rate=0.000001,\n",
    "                               num_train_epochs=4\n",
    "                               )\n",
    "trainer = transformers.Trainer(\n",
    "                                model=model,\n",
    "                                tokenizer=tokenizer,\n",
    "                                compute_metrics=compute_metrics,\n",
    "                                train_dataset=train_dataset,\n",
    "                                eval_dataset=val_dataset,\n",
    "                                args = train_args\n",
    "                                )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hxq6SMZh_5X"
   },
   "source": [
    "## Test model\n",
    "\n",
    "After training the model, we can see the performance of our model on the test set, which are samples that the model has not previously seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6dGSxPviBXm"
   },
   "outputs": [],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayrhgrZliZ55"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEieilNqidoB"
   },
   "source": [
    "# Now, it's your turn\n",
    "\n",
    "We use CTCF binding site as example but you can also use it for many other tasks such as promoter prediction, structural variants, integration sites of transposable elements, other binding sites, and many more."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
